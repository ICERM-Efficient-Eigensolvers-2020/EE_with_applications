{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Web Crawler with Directed Graph Generator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#copy right to: https://www.thepythoncode.com/article/extract-all-website-links-python\n",
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import colorama\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# init the colorama module\n",
    "colorama.init()\n",
    "GREEN = colorama.Fore.GREEN\n",
    "GRAY = colorama.Fore.LIGHTBLACK_EX\n",
    "RESET = colorama.Fore.RESET\n",
    "\n",
    "# initialize the set of links (unique links)\n",
    "internal_urls = set()\n",
    "external_urls = set()\n",
    "\n",
    "def is_valid(url):\n",
    "    \"\"\"\n",
    "    Checks whether `url` is a valid URL.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Given an url, we would love to dig the subpages of it. By using\n",
    "beatifulsoup, we could crawl all the relative pages."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_all_website_links(url, max_urls):\n",
    "    \"\"\"\n",
    "    Returns all URLs that is found on `url` in which it belongs to the same website\n",
    "    \"\"\"\n",
    "    global diG\n",
    "    global url_dict\n",
    "    global idx\n",
    "    global root\n",
    "    # all URLs of `url`\n",
    "    urls = set()\n",
    "    children = set()\n",
    "    # domain name of the URL without the protocol\n",
    "    domain_name = urlparse(url).netloc\n",
    "    parent = url_dict[url]\n",
    "    soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "    for a_tag in soup.findAll(\"a\"):\n",
    "        href = a_tag.attrs.get(\"href\")\n",
    "        if href == \"\" or href is None:\n",
    "            # href empty tag\n",
    "            continue\n",
    "\n",
    "        # join the URL if it's relative (not absolute link)\n",
    "        href = urljoin(url, href)\n",
    "        parsed_href = urlparse(href)\n",
    "        # remove URL GET parameters, URL fragments, etc.\n",
    "        href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path\n",
    "        if not is_valid(href):\n",
    "            # not a valid URL\n",
    "            continue\n",
    "\n",
    "        if domain_name not in href:\n",
    "            # external link\n",
    "            if href not in external_urls:\n",
    "                #print(f\"{GRAY}[!] External link: {href}{RESET}\")\n",
    "                external_urls.add(href)\n",
    "            continue\n",
    "\n",
    "        #found out the internal link\n",
    "\n",
    "        #new child\n",
    "        #add some filters to make this rank more meaningful\n",
    "\n",
    "        if href.endswith('.pdf'):\n",
    "            continue\n",
    "        if href not in url_dict.keys():\n",
    "            idx = idx + 1\n",
    "            if idx >= max_urls:\n",
    "                continue\n",
    "            diG.add_node(idx)\n",
    "            url_dict[href] = idx\n",
    "\n",
    "            #prepare new internal_urls to crawl\n",
    "            if urls != url:\n",
    "                urls.add(href)\n",
    "            else:\n",
    "                print(\"WHHHHHAAAAT\")\n",
    "            internal_urls.add(href)\n",
    "        #connection added\n",
    "        child = url_dict[href]\n",
    "        diG.add_edge(parent, child)\n",
    "        children.add(href)\n",
    "        #print(f\"{GREEN}[*] Internal link: {href}{RESET}\")\n",
    "\n",
    "    return urls, children"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use recursion to finish this crawling procedure for the\n",
    "starting page."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# number of urls visited so far will be stored here\n",
    "total_urls_visited = 0\n",
    "\n",
    "def crawl(url, max_urls):\n",
    "\n",
    "    \"\"\"\n",
    "    Crawls a web page and extracts all links.\n",
    "    You'll find all links in `external_urls` and `internal_urls` global set variables.\n",
    "    params:\n",
    "        max_urls (int): number of max urls to crawl, default is 30.\n",
    "    \"\"\"\n",
    "    global diG\n",
    "    global url_dict\n",
    "    global total_urls_visited\n",
    "    global idx\n",
    "\n",
    "    total_urls_visited += 1\n",
    "\n",
    "    if total_urls_visited > max_urls:\n",
    "        print(\"!!!Let's dance!!!!\")\n",
    "    else:\n",
    "        links, children = get_all_website_links(url, max_urls)\n",
    "        for link in links:\n",
    "            crawl(link, max_urls=max_urls)\n",
    "\n",
    "def scraper(url, max_urls):\n",
    "    global idx\n",
    "    global diG\n",
    "    global url_dict\n",
    "    global total_urls_visited\n",
    "    global root\n",
    "    diG = nx.DiGraph()\n",
    "    url_dict = {}\n",
    "    idx = 0\n",
    "\n",
    "    url_dict[url] = idx\n",
    "    diG.add_node(idx)\n",
    "    root = url\n",
    "    crawl(url, max_urls=max_urls)\n",
    "\n",
    "    print(\"[+] Total Internal links:\", len(internal_urls))\n",
    "    print(\"[+] Total External links:\", len(external_urls))\n",
    "    print(\"[+] Total URLs:\", len(external_urls) + len(internal_urls))\n",
    "\n",
    "    domain_name = urlparse(url).netloc\n",
    "\n",
    "    # save the internal links to a file\n",
    "    with open(f\"{domain_name}_internal_links.txt\", \"w\") as f:\n",
    "        for internal_link in internal_urls:\n",
    "            print(internal_link.strip(), file=f)\n",
    "\n",
    "    # save the external links to a file\n",
    "    with open(f\"{domain_name}_external_links.txt\", \"w\") as f:\n",
    "        for external_link in external_urls:\n",
    "            print(external_link.strip(), file=f)\n",
    "\n",
    "    #adjacency matrix\n",
    "    A = nx.to_numpy_matrix(diG)\n",
    "    return A, diG, url_dict\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}